{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from nn.rrdbunet import UNetRRDB2\n",
    "from utils.dataloader import DatasetConfig, CC_Dataset, NumpyDataset\n",
    "from nn.trainer import Trainer\n",
    "from nn.hooks import EarlyStopping\n",
    "from nn.losses import CombinedLoss\n",
    "from utils.transforms import norm_ccmat\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two options for datasets. Either Numpy or directly cooler. \n",
    "# Numpy is more straight forward, but requires much more storage, since cooler data loads on demand\n",
    "# Some sample data can be downloaded (check Readme). \n",
    "# If you want to generate your own data from cooler use: ccut/data_prep/convert_and_normalize_v3.py\n",
    "\n",
    "df_numpy = NumpyDataset(\n",
    "    # sample list based on genomic coordinates. Can be generated: ccut/data_prep/create_sliding_window_coor.v2.py\n",
    "    sample_cordinates_file=\"../data/chr19-22_40x40x50k_nozeroes.csv\",\n",
    "    # pair of cc data in low and high resolution (input, target)\n",
    "    highres_path=\"../data/porec-perc999-hr_cutoff-norm-chr19-22-50k-chromosomes.npz\",\n",
    "    lowres_path=\"../data/porec-4x-perc999-hr_cutoff-norm-chr19-22-50k-chromosomes.npz\",\n",
    "    # bin resolution \n",
    "    resolution=50_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup model\n",
    "unet = UNetRRDB2(in_channels=1, out_channels=1, features=[64, 128, 256, 512, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate trainer. Can be done using ccdataset class as well as numpydataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    df_numpy, batch_size=4, num_workers=0, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/operator/mambaforge/envs/ccut-test/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize your hooks\n",
    "hook_classes = [EarlyStopping]\n",
    "\n",
    "# Define hook arguments\n",
    "hook_args = {}\n",
    "\n",
    "# Set loss function / can be custom if based on abstract class provided in ccut/nn/losses.py\n",
    "loss = CombinedLoss(window_size=15)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=unet,\n",
    "    loss_function=loss,\n",
    "    # use grad scaler\n",
    "    grad_scaler=torch.cuda.amp.GradScaler(),\n",
    "    # hooks and arguments for hooks\n",
    "    hooks=hook_classes,\n",
    "    hook_args=hook_args,\n",
    "    # optimizer as Adam and parameters\n",
    "    optim_class=torch.optim.Adam,\n",
    "    optim_params={\"lr\": 5e-5, \"betas\": (0.0, 0.9), \"weight_decay\": 1e-4},\n",
    "    # use mixed precision ? \n",
    "    mixed_precision=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:01:04,738 - INFO - Starting training\n",
      "/home/operator/mambaforge/envs/ccut-test/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "2024-06-08 14:01:07,633 - INFO - 0.644659\n",
      "2024-06-08 14:01:09,968 - INFO - 0.357500\n",
      "2024-06-08 14:01:12,188 - INFO - 0.246894\n",
      "2024-06-08 14:01:14,377 - INFO - 0.157305\n",
      "2024-06-08 14:01:16,525 - INFO - 0.125291\n"
     ]
    }
   ],
   "source": [
    "# Setup logging: Intended for vanilla python scipts not nb's\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "\n",
    "# start training for 10 epochs\n",
    "trainer.train(train_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Saving and exporting + Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccut-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
